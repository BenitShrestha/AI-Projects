# Workflow Involved 

--- Training Part ---
1. Importing Necessary Libraries and Modules

2. Initializing Variables and Loading Data

3. Tokenizing and Lemmatizing Patterns

    Code:
    for intent in intents['intents']:
        for pattern in intent['patterns']:
            word_list = nltk.word_tokenize(pattern)
            words.extend(word_list)
            documents.append((word_list, intent['tag']))
            if intent['tag'] not in classes:
                classes.append(intent['tag'])

    Example
    Suppose the JSON contains:

{
    "intents": [
        {
            "tag": "products",
            "patterns": ["What products do you offer?", "Can you tell me about your products?", "What kind of products do you have?"],
            "responses": ["We offer a variety of products including electronics, clothing, and accessories.", "Our products range from tech gadgets to fashion items."]
        },
        {
            "tag": "shop",
            "patterns": ["Where is your shop located?", "What are your shop hours?", "Do you have a physical store?"],
            "responses": ["Our shop is located at [address].", "Our shop hours are from [hours].", "Yes, we have a physical store at [location]."]
        },
        {
            "tag": "hours",
            "patterns": ["What are your opening hours?", "When do you open and close?", "What time do you close today?"],
            "responses": ["We are open from [opening time] to [closing time] every day.", "Our hours are [opening hours].", "Today, we close at [closing time]."]
        }
    ]
}


    After processing:

    words: ['What', 'products', 'do', 'you', 'offer', 'Can', 'tell', 'me', 'about', 'your', 'What', 'kind', 'of', 'products', 'do', 'you', 'have', 'Where', 'is', 'your', 'shop', 'located', 'What', 'are', 'your', 'shop', 'hours', 'Do', 'you', 'have', 'a', 'physical', 'store', 'What', 'are', 'your', 'opening', 'hours', 'When', 'do', 'you', 'open', 'and', 'close', 'What', 'time', 'do', 'you', 'close', 'today']
    classes: ['products', 'shop', 'hours']
    documents: [(['What', 'products', 'do', 'you', 'offer'], 'products'), (['Can', 'you', 'tell', 'me', 'about', 'your', 'products'], 'products'), (['What', 'kind', 'of', 'products', 'do', 'you', 'have'], 'products'), (['Where', 'is', 'your', 'shop', 'located'], 'shop'), (['What', 'are', 'your', 'shop', 'hours'], 'shop'), (['Do', 'you', 'have', 'a', 'physical', 'store'], 'shop'), (['What', 'are', 'your', 'opening', 'hours'], 'hours'), (['When', 'do', 'you', 'open', 'and', 'close'], 'hours'), (['What', 'time', 'do', 'you', 'close', 'today'], 'hours')]

4. Lemmatizing and Sorting Words and Classes

    Code:
    words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]
    words = sorted(set(words))
    classes = sorted(set(classes))

    Example

        words (after lemmatization and sorting): ['about', 'a', 'and', 'are', 'can', 'close', 'do', 'have', 'hours', 'is', 'kind', 'located', 'me', 'offer', 'of', 'open', 'physical', 'products', 'shop', 'store', 'tell', 'time', 'today', 'what', 'when', 'where', 'you', 'your']
        classes (after sorting): ['hours', 'products', 'shop']

5. Saving Preprocessed Data to Pickle Files

6. Creating Training Data

Code:
training = []
output_empty = [0] * len(classes)

for document in documents:
    bag = []
    word_patterns = document[0]
    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]
    for word in words:
        bag.append(1) if word in word_patterns else bag.append(0)
    
    output_row = list(output_empty)
    output_row[classes.index(document[1])] = 1
    training.append([bag, output_row])

random.shuffle(training)
training = np.array(training, dtype=object)

train_x = np.array([item[0] for item in training], dtype=np.float32)
train_y = np.array([item[1] for item in training], dtype=np.float32)

Example

For the first document (['What', 'products', 'do', 'you', 'offer', '?'], 'products'):

    word_patterns: ['what', 'product', 'do', 'you', 'offer'] (after lemmatization and lowercasing)
    bag: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0] (presence of words in words list)
    output_row: [0, 1, 0] (corresponding to the 'products' class)

For the second document (['Where', 'is', 'your', 'shop', 'located', '?'], 'shop'):

    word_patterns: ['where', 'is', 'your', 'shop', 'located']
    bag: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1]
    output_row: [0, 0, 1] (corresponding to the 'shop' class)

Example might look same as:
[
    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0][0, 1, 0] ...
]

--- Chatbot Part ---
1. Imports and Initialization

2. Load and Initialize Data - JSON file, Pickle files: Words, Classes, Model

3. Define Helper Functions

    Clean Up Sentence: Tokenize and Lemmatize Input and return
        Code:
        def clean_up_sentence(sentence):
            sentence_words = nltk.word_tokenize(sentence)
            sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]
            return sentence_words

    Bag of Words: Calls Clean Up Sentence, Creates Bag of zeroes of length Words, If words of input sentence in Words, Assign 1 to respective index of Bag, Return np.array
        Code:
        def bag_of_words(sentence):
            sentence_words = clean_up_sentence(sentence)
            bag = [0] * len(words)
            for w in sentence_words:
                for i, word in enumerate(words):
                    if word == w:
                        bag[i] = 1
            return np.array(bag)

    Predict Class: Generate a Bag of Words, Predict probabilities using model, Create a list Results containing index and probabilities above a threshold value, Sort from highest to lowest
                   Returns Return_list: a dictionary containing predicted tags and their probabilities
        Code:
        def predict_class(sentence):
            bow = bag_of_words(sentence)
            res = model.predict(np.array([bow]))[0]
            ERROR_THRESHOLD = 0.25
            results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD] # Index (0, 1, 2, ..) and Probabilities
            
            results.sort(key = lambda x: x[1], reverse = True)
            return_list = []
            for r in results:
                return_list.append({'intent': classes[r[0]], 'probability': str(r[1])})
            return return_list

    Get Response: Extracts predicted tag from intents_list, Loads list of intents from JSON, If matching tag is found, Random Response is chosen from JSON file, Returned as output
        Code:
        def get_response(intents_list, intents_json): 
            tag = intents_list[0]['intent']
            list_of_intents = intents_json['intents']
            result = "Sorry, I didn't get that. Please try again."
            for i in list_of_intents:
                if i['tag'] == tag:
                    result = random.choice(i['responses'])
                    break
            return result

4. Main Loop
    Code:
    print('Chatbot is running!')

    while True:
        message = input("")
        ints = predict_class(message)
        if ints:
            res = get_response(ints, intents) # Intents here is JSON file
        else:
            res = "Sorry I got confused"
        print(res)

--- Example Workflow ---

Let's go through an example to see how each part works:

Input Provided:
message = "What products do you offer?"

Clean Up Sentence
sentence_words = clean_up_sentence(message)
# sentence_words = ['what', 'product', 'do', 'you', 'offer']

Bag of Words
bow = bag_of_words(message)
# words = ['about', 'a', 'and', 'are', 'can', 'close', 'do', 'have', 'hours', 'how', 'is', 'kind', 'located', 'me', 'of', 'offer', 'open', 'physical', 'product', 'shop', 'store', 'tell', 'time', 'today', 'what', 'where', 'when', 'you', 'your']
# bow = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]

Predict Class
ints = predict_class(message)
# Assume model prediction output: [0.1, 0.9, 0.0]
# ERROR_THRESHOLD = 0.25
# results = [[1, 0.9]]
# return_list = [{'intent': 'products', 'probability': '0.9'}]

Get Responses
res = get_response(ints, intents)
# intents_list = [{'intent': 'products', 'probability': '0.9'}]
# result = "We offer a variety of products including electronics, clothing, and accessories."

Print Response
print(res)
# Output: "We offer a variety of products including electronics, clothing, and accessories."