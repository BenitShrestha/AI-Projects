# Steps involved with examples

1. Import Necessary Libraries and Modules

2. Intializing Variables and Loading Data

3. Tokenizing and Lemmatizing Patterns

    Code:
    for intent in intents['intents']:
        for pattern in intent['patterns']:
            word_list = nltk.word_tokenize(pattern)
            words.extend(word_list)
            documents.append((word_list, intent['tag']))
            if intent['tag'] not in classes:
                classes.append(intent['tag'])

    Suppose a JSON contains:
    {
    "intents": [
        {
        "tag": "greeting",
        "patterns": ["Hello", "Hi", "Greetings"],
        "responses": ["Hello!", "Hi there!"]
        },
        {
        "tag": "goodbye",
        "patterns": ["Bye", "See you later", "Goodbye"],
        "responses": ["Goodbye!", "See you later!"]
        }
    ]
    }

    After processing:

        words: ['Hello', 'Hi', 'Greetings', 'Bye', 'See', 'you', 'later', 'Goodbye']
        classes: ['greeting', 'goodbye']
        documents: [(['Hello'], 'greeting'), (['Hi'], 'greeting'), (['Greetings'], 'greeting'), (['Bye'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Goodbye'], 'goodbye')]

4. Lemmatizing and Sorting Words and Classes

    Code:
    words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]
    words = sorted(set(words))
    classes = sorted(set(classes))

    Example
        Here, lemmatization isn't really done but if JSON had something like "working", "works" - It would be lemmatized to "work"
        words (after lemmatization and sorting): ['Bye', 'Goodbye', 'Greetings', 'Hello', 'Hi', 'See', 'later', 'you']
        classes (after sorting): ['goodbye', 'greeting']

5. Saving Preprocessed Data to Pickle Files

6. Creating Training Data

    Code:
    training = []
    output_empty = [0] * len(classes)

    for document in documents:
        bag = []
        word_patterns = document[0]
        word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]
        for word in words:
            bag.append(1) if word in word_patterns else bag.append(0)
        
        output_row = list(output_empty)
        output_row[classes.index(document[1])] = 1 # Finds index
        training.append([bag, output_row])

    random.shuffle(training)
    training = np.array(training, dtype=object)

    train_x = np.array([item[0] for item in training], dtype=np.float32)
    train_y = np.array([item[1] for item in training], dtype=np.float32)

    Example

    For the first document (['Hello'], 'greeting'):

        word_patterns: ['hello'] (after lemmatization and lowercasing)
        bag: [0, 0, 0, 1, 0, 0, 0, 0] (only 'Hello' is present, so the fourth position is 1)
        output_row: [0, 1] (corresponding to the 'greeting' class)

    For the second document (['Bye'], 'goodbye'):

        word_patterns: ['bye']
        bag: [1, 0, 0, 0, 0, 0, 0, 0] (only 'Bye' is present, so the first position is 1)
        output_row: [1, 0] (corresponding to the 'goodbye' class)

    After processing all documents, the training list might look like this:

    Code:
    [
        [[0, 0, 0, 1, 0, 0, 0, 0], [0, 1]],
        [[1, 0, 0, 0, 0, 0, 0, 0], [1, 0]],
        # ... more training pairs
    ]

    After shuffling and converting to NumPy arrays:

        train_x: Feature vectors (bag-of-words representations), item[0]
        train_y: One-hot encoded labels, item[1]