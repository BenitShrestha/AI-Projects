# Workflow Involved 

--- Training Part ---
1. Importing Necessary Libraries and Modules

2. Initializing Variables and Loading Data

3. Tokenizing and Lemmatizing Patterns

    Code:
    for intent in intents['intents']:
        for pattern in intent['patterns']:
            word_list = nltk.word_tokenize(pattern)
            words.extend(word_list)
            documents.append((word_list, intent['tag']))
            if intent['tag'] not in classes:
                classes.append(intent['tag'])

    Example
    Suppose the JSON contains:

{
    "intents": [
        {
            "tag": "products",
            "patterns": ["What products do you offer?", "Can you tell me about your products?", "What kind of products do you have?"],
            "responses": ["We offer a variety of products including electronics, clothing, and accessories.", "Our products range from tech gadgets to fashion items."]
        },
        {
            "tag": "shop",
            "patterns": ["Where is your shop located?", "What are your shop hours?", "Do you have a physical store?"],
            "responses": ["Our shop is located at [address].", "Our shop hours are from [hours].", "Yes, we have a physical store at [location]."]
        },
        {
            "tag": "hours",
            "patterns": ["What are your opening hours?", "When do you open and close?", "What time do you close today?"],
            "responses": ["We are open from [opening time] to [closing time] every day.", "Our hours are [opening hours].", "Today, we close at [closing time]."]
        }
    ]
}


    After processing:

    words: ['What', 'products', 'do', 'you', 'offer', 'Can', 'tell', 'me', 'about', 'your', 'What', 'kind', 'of', 'products', 'do', 'you', 'have', 'Where', 'is', 'your', 'shop', 'located', 'What', 'are', 'your', 'shop', 'hours', 'Do', 'you', 'have', 'a', 'physical', 'store', 'What', 'are', 'your', 'opening', 'hours', 'When', 'do', 'you', 'open', 'and', 'close', 'What', 'time', 'do', 'you', 'close', 'today']
    classes: ['products', 'shop', 'hours']
    documents: [(['What', 'products', 'do', 'you', 'offer'], 'products'), (['Can', 'you', 'tell', 'me', 'about', 'your', 'products'], 'products'), (['What', 'kind', 'of', 'products', 'do', 'you', 'have'], 'products'), (['Where', 'is', 'your', 'shop', 'located'], 'shop'), (['What', 'are', 'your', 'shop', 'hours'], 'shop'), (['Do', 'you', 'have', 'a', 'physical', 'store'], 'shop'), (['What', 'are', 'your', 'opening', 'hours'], 'hours'), (['When', 'do', 'you', 'open', 'and', 'close'], 'hours'), (['What', 'time', 'do', 'you', 'close', 'today'], 'hours')]

4. Lemmatizing and Sorting Words and Classes

    Code:
    words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]
    words = sorted(set(words))
    classes = sorted(set(classes))

    Example

        words (after lemmatization and sorting): ['about', 'a', 'and', 'are', 'can', 'close', 'do', 'have', 'hours', 'is', 'kind', 'located', 'me', 'offer', 'of', 'open', 'physical', 'products', 'shop', 'store', 'tell', 'time', 'today', 'what', 'when', 'where', 'you', 'your']
        classes (after sorting): ['hours', 'products', 'shop']

5. Saving Preprocessed Data to Pickle Files

6. Creating Training Data

Code:
training = []
output_empty = [0] * len(classes)

for document in documents:
    bag = []
    word_patterns = document[0]
    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]
    for word in words:
        bag.append(1) if word in word_patterns else bag.append(0)
    
    output_row = list(output_empty)
    output_row[classes.index(document[1])] = 1
    training.append([bag, output_row])

random.shuffle(training)
training = np.array(training, dtype=object)

train_x = np.array([item[0] for item in training], dtype=np.float32)
train_y = np.array([item[1] for item in training], dtype=np.float32)

Example

For the first document (['What', 'products', 'do', 'you', 'offer', '?'], 'products'):

    word_patterns: ['what', 'product', 'do', 'you', 'offer'] (after lemmatization and lowercasing)
    bag: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0] (presence of words in words list)
    output_row: [0, 1, 0] (corresponding to the 'products' class)

For the second document (['Where', 'is', 'your', 'shop', 'located', '?'], 'shop'):

    word_patterns: ['where', 'is', 'your', 'shop', 'located']
    bag: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1]
    output_row: [0, 0, 1] (corresponding to the 'shop' class)

Example might look same as:
[
    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0][0, 1, 0] ...
]

--- Chatbot Part ---
