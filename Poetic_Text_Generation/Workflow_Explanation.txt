# Workflow Explanation 
1. Data Loading and Preprocessing 
    Code: 
    import tensorflow as tf
    import numpy as np

    def load_data():
        """ Loads Shakespearean text data and prepares character mappings. """
        filepath = tf.keras.utils.get_file('shakespeare.txt', 'https://www.gutenberg.org/files/100/100-0.txt')
        text = open(filepath, 'rb').read().decode(encoding='utf-8').lower()[:500000]
        
        characters = sorted(set(text))
        char_to_index = {char: idx for idx, char in enumerate(characters)}
        index_to_char = {idx: char for idx, char in enumerate(characters)}
        
        return text, char_to_index, index_to_char, characters

    Explanation:
    Loaded Shakespeare's text from Gutenberg and limited text used to 500k characters
    Sorted them and made a separate list of unique characters (a, b, c, ?)
    One to one mapping of characters with indices, later used for conversion 

2. Data Preparation 
    Code:
    def prepare_data(text, char_to_index):
        """ Prepares input-output pairs for the model training. """
        SEQ_LENGTH = 100
        STEP_SIZE = 10
        
        sentences = []
        next_characters = []
        
        for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE):
            sentences.append(text[i: i + SEQ_LENGTH])
            next_characters.append(text[i + SEQ_LENGTH])
        
        x = np.zeros((len(sentences), SEQ_LENGTH, len(char_to_index)), dtype=np.bool)
        y = np.zeros((len(sentences), len(char_to_index)), dtype=np.bool)
        
        for i, sentence in enumerate(sentences):
            for t, char in enumerate(sentence):
                x[i, t, char_to_index[char]] = 1 # See Example for better understanding
            y[i, char_to_index[next_characters[i]]] = 1
        
        return x, y

        Explanation:
        First loop, goes through length of text in Step Size of 10
        100 characters are assumed a sentence 
        Next character of each sentence is stored in Next Characters list
        Assume a string "goodbye" for Sequence Length = 3, Step Size = 2
        Sentences: ['goo', 'odb', 'bye']
        Next Characters: ['d', 'b', '']

        One Hot Encoding of x, y and Second loop:
        Sets up x and y as np arrays of required dimensions (x: 3D, y:2D) and fills with zeroes

        x : Number of Sequences, Length of each Sequence, No. of unique characters in dataset
        y: Number of Sequences, No. of unique characters

        For each char in each sentence of Sentences, One Hot Encodes x[sentence no., character no., unique mapping of char in sentence] = 1
        y[sentence no., unique mapping of next characters]

        Example:
        Suppose:
        sentences = ['abc', 'def']
        next_characters = ['x', 'y']
        char_to_index = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'x': 6, 'y': 7}

        x would be a 3D array with dimensions (2, 3, 8):
    [
        [
            [1, 0, 0, 0, 0, 0, 0, 0],  # 'a'
            [0, 1, 0, 0, 0, 0, 0, 0],  # 'b'
            [0, 0, 1, 0, 0, 0, 0, 0]   # 'c'
        ],
        [
            [0, 0, 0, 1, 0, 0, 0, 0],  # 'd'
            [0, 0, 0, 0, 1, 0, 0, 0],  # 'e'
            [0, 0, 0, 0, 0, 1, 0, 0]   # 'f'
        ]
    ]

        y would be a 2D array with dimensions (2, 8):
    [
        [0, 0, 0, 0, 0, 0, 1, 0],  # 'x'
        [0, 0, 0, 0, 0, 0, 0, 1]   # 'y'
    ]

3. Model Building 
    Sequential model, Bidirectional LSTM with 256 neurons, Softmax was used to generate probabilities for all characters

4. Model Training
    Model was trained for 50 epochs with a batch size of 256. Adam optimizer was used, Categorical Crossentropy as loss function