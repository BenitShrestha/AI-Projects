# Workflow Explanation 
1. Data Loading and Preprocessing 
    Code: 
    import tensorflow as tf
    import numpy as np

    def load_data():
        """ Loads Shakespearean text data and prepares character mappings. """
        filepath = tf.keras.utils.get_file('shakespeare.txt', 'https://www.gutenberg.org/files/100/100-0.txt')
        text = open(filepath, 'rb').read().decode(encoding='utf-8').lower()[:500000]
        
        characters = sorted(set(text))
        char_to_index = {char: idx for idx, char in enumerate(characters)}
        index_to_char = {idx: char for idx, char in enumerate(characters)}
        
        return text, char_to_index, index_to_char, characters

    Explanation:
    Loaded Shakespeare's text from Gutenberg and limited text used to 500k characters
    Sorted them and made a separate list of unique characters (a, b, c, ?)
    One to one mapping of characters with indices, later used for conversion 

2. Data Preparation 
    Code:
    def prepare_data(text, char_to_index):
        """ Prepares input-output pairs for the model training. """
        SEQ_LENGTH = 100
        STEP_SIZE = 10
        
        sentences = []
        next_characters = []
        
        for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE):
            sentences.append(text[i: i + SEQ_LENGTH])
            next_characters.append(text[i + SEQ_LENGTH])
        
        x = np.zeros((len(sentences), SEQ_LENGTH, len(char_to_index)), dtype=np.bool)
        y = np.zeros((len(sentences), len(char_to_index)), dtype=np.bool)
        
        for i, sentence in enumerate(sentences):
            for t, char in enumerate(sentence):
                x[i, t, char_to_index[char]] = 1 # See Example for better understanding
            y[i, char_to_index[next_characters[i]]] = 1
        
        return x, y

        Explanation:
        First loop, goes through length of text in Step Size of 10
        100 characters are assumed a sentence 
        Next character of each sentence is stored in Next Characters list
        Assume a string "goodbye" for Sequence Length = 3, Step Size = 2
        Sentences: ['goo', 'odb', 'bye']
        Next Characters: ['d', 'b', '']

        One Hot Encoding of x, y and Second loop:
        Sets up x and y as np arrays of required dimensions (x: 3D, y:2D) and fills with zeroes

        x : Number of Sequences, Length of each Sequence, No. of unique characters in dataset
        y: Number of Sequences, No. of unique characters

        For each char in each sentence of Sentences, One Hot Encodes x[sentence no., character no., unique mapping of char in sentence] = 1
        y[sentence no., unique mapping of next characters]

        Example:
        Suppose:
        sentences = ['abc', 'def']
        next_characters = ['x', 'y']
        char_to_index = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'x': 6, 'y': 7}

        x would be a 3D array with dimensions (2, 3, 8):
    [
        [
            [1, 0, 0, 0, 0, 0, 0, 0],  # 'a'
            [0, 1, 0, 0, 0, 0, 0, 0],  # 'b'
            [0, 0, 1, 0, 0, 0, 0, 0]   # 'c'
        ],
        [
            [0, 0, 0, 1, 0, 0, 0, 0],  # 'd'
            [0, 0, 0, 0, 1, 0, 0, 0],  # 'e'
            [0, 0, 0, 0, 0, 1, 0, 0]   # 'f'
        ]
    ]

        y would be a 2D array with dimensions (2, 8):
    [
        [0, 0, 0, 0, 0, 0, 1, 0],  # 'x'
        [0, 0, 0, 0, 0, 0, 0, 1]   # 'y'
    ]

3. Model Building 
    Sequential model, Bidirectional LSTM with 256 neurons, Softmax was used to generate probabilities for all characters

4. Model Training
    Model was trained for 50 epochs with a batch size of 256. Adam optimizer was used, Categorical Crossentropy as loss function

5. Text Generation
    Code:
    def generate_text(model, text, char_to_index, index_to_char, characters, length=500, temperature=1.0):
        """ Generates text using the trained model and user-defined parameters. """
        start_index = np.random.randint(0, len(text) - SEQ_LENGTH - 1)
        generated_text = text[start_index: start_index + SEQ_LENGTH]
        
        for _ in range(length):
            x_pred = np.zeros((1, SEQ_LENGTH, len(characters)))
            for t, char in enumerate(generated_text):
                x_pred[0, t, char_to_index[char]] = 1
            
            preds = model.predict(x_pred, verbose=0)[0]
            next_index = sample(preds, temperature)
            next_char = index_to_char[next_index]
            
            generated_text += next_char
            generated_text = generated_text[1:]
        
        return generated_text

    Explanation:
    Randomly selects a starting point 
    Prints characters up to Sequence length from that point 
    Then loop iterates for given length or 500 default 
    Prepare input value sent to model for predictions 
    np array of zeroes, One Hot Encode the previously generated text 
    Send for prediction, get index for next character using Sample function (Keras Documentation)
    Get next character using index (Using index to char mapping dictionary)
    Append to generated text, return at end 

    Example:
    Assume:
    text = "The quick brown fox jumps over the lazy dog. "
    start_index = 5  # Example random starting index
    SEQ_LENGTH = 20
    generated_text = "quick brown fox ju"

    Text Generation:
    length = 100
    temperature = 0.5
    for _ in range(length):
        # Inside the loop, it repeatedly predicts and appends characters

    Preparing Input:
    x_pred = np.zeros((1, 20, 29))  # Example size for one-hot encoding
    generated_text = "quick brown fox ju"

    Predicted Probabilities:
    preds = [0.2, 0.4, 0.1, ...]  # Example predicted probabilities for next character

    Generated Text: Assume temperature of 0.5
    "quick brown fox jumps over the lazy dog. he brown fox jumps over the lazy dog. he brown fox jumps over the lazy dog. "

6. Main Function 
    Code:
    def main():
        text, char_to_index, index_to_char, characters = load_data()
        x, y = prepare_data(text, char_to_index)
        
        model = build_model(input_shape=(x.shape[1], x.shape[2]), num_classes=len(characters))
        model = train_model(model, x, y)
        
        generated_text = generate_text(model, text, char_to_index, index_to_char, characters)
        print("Generated Text:\n", generated_text)

    Explanation:
    Gets text, mapping dictionaries from Load Data function
    Gets x and y from Prepare data function 
    Builds Model, Generates text using respective function 
    Prints output

7. Run Module
    Essentially, runs the code of Main program module
    Loads saved model (Models were trained on Google Colab so might not run locally)
    Accepts length - text to generate, temperature - controls randomness and diversity of generated text, higher = more random

    A higher temperature increases the likelihood of selecting less probable characters, making the generated text more diverse and unpredictable.
    Conversely, a lower temperature makes the model more confident in its predictions, favoring characters with higher probabilities and generating more deterministic text.

    Temperature scaling affects how probabilities are sampled from a distribution. For preds = [0.3, 0.2, 0.1, 0.4]:

    Temperature 0.1: Likely outcome is [0, 0, 0, 1], favoring the highest probability (0.4).
    Temperature 0.5: Outcome could still be [0, 0, 0, 1], with some chance for other indices.
    Temperature 1.0: Balanced sampling across [0, 1, 2, 3], more likely towards index 3 due to higher probability.
    Temperature 1.5: Sampling across [0, 1, 2, 3], with index 3 still favored.
    Temperature 2.0: Uniform sampling across [0, 1, 2, 3], slightly favoring index 3.